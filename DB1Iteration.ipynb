{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPprBx/Sby43z1/1YQPhXBz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devdaveddev/DebateBOT/blob/main/DB1Iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b8vGMRczO5b"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import openai\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from transformers import pipeline, BertForSequenceClassification, BertTokenizer\n",
        "from textblob import TextBlob\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Initialize NLP models\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Load BERT model for sentiment analysis\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "\n",
        "def bert_sentiment_analysis(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    outputs = bert_model(**inputs)\n",
        "    scores = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    sentiment_score = torch.argmax(scores).item()\n",
        "    return sentiment_score\n",
        "\n",
        "# OpenAI API Key (Replace with your own)\n",
        "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "# Function to fetch data from Wikipedia API\n",
        "def fetch_wikipedia_data(topic):\n",
        "    url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{topic}\"\n",
        "    response = requests.get(url).json()\n",
        "    return response.get(\"extract\", \"No information found.\")\n",
        "\n",
        "# Function to fetch arguments from Reddit API (requires authentication)\n",
        "def fetch_reddit_data(topic):\n",
        "    headers = {\"User-Agent\": \"AI-Debater\"}\n",
        "    url = f\"https://www.reddit.com/r/debate/comments.json?q={topic}&sort=top\"\n",
        "    response = requests.get(url, headers=headers).json()\n",
        "    posts = [post['data']['title'] for post in response['data']['children'][:5]]\n",
        "    return posts\n",
        "\n",
        "# Backpropagation-based Weight Adjustment\n",
        "def update_weights(weights, learning_rate=0.01):\n",
        "    gradient = np.random.randn(len(weights))  # Simulated gradient update\n",
        "    new_weights = weights - learning_rate * gradient\n",
        "    return np.clip(new_weights, 0, 1)  # Ensure weights remain valid\n",
        "\n",
        "# Function to generate debate arguments using OpenAI, Wikipedia, and Reddit data\n",
        "def generate_human_like_arguments(topic, debate_type):\n",
        "    wikipedia_info = fetch_wikipedia_data(topic)\n",
        "    reddit_info = fetch_reddit_data(topic)\n",
        "\n",
        "    # Assigning Weights based on Debate Type\n",
        "    weights = np.array([\n",
        "        [0.7, 0.2, 0.1],  # Educated: More Wikipedia, less Reddit, least AI\n",
        "        [0.0, 0.0, 1.0],  # Radical: Only AI-based extreme arguments\n",
        "        [0.2, 0.7, 0.1]   # Casual: More Reddit, some Wikipedia, little AI\n",
        "    ])\n",
        "    weight_labels = {\"educated\": 0, \"radical\": 1, \"casual\": 2}\n",
        "    w_wiki, w_reddit, w_ai = weights[weight_labels.get(debate_type, 0)]\n",
        "\n",
        "    # Update weights dynamically using backpropagation technique\n",
        "    weights[weight_labels.get(debate_type, 0)] = update_weights(weights[weight_labels.get(debate_type, 0)])\n",
        "\n",
        "    combined_context = f\"Wikipedia Info: {wikipedia_info * w_wiki}\\n\\nReddit Insights: {reddit_info * w_reddit}\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You generate arguments by merging factual information from Wikipedia, Reddit, and OpenAI-generated insights, ensuring a human-like debate experience.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Debate this topic with strong reasoning: {topic}\\n\\n{combined_context}\"}\n",
        "        ]\n",
        "    )\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "# Function to analyze sentiment & bias using BERT\n",
        "def analyze_bias(user_input):\n",
        "    sentiment_score = bert_sentiment_analysis(user_input)\n",
        "    if sentiment_score >= 4:\n",
        "        return \"Highly Positive (Potential Bias)\"\n",
        "    elif sentiment_score <= 1:\n",
        "        return \"Highly Negative (Potential Bias)\"\n",
        "    return \"Neutral\"\n",
        "\n",
        "# Function to simulate debate\n",
        "def ai_debate():\n",
        "    debate_type = input(\"Choose debate style (educated, radical, casual): \")\n",
        "    topic = input(\"Enter a topic to debate: \")\n",
        "\n",
        "    print(\"\\nFetching arguments...\\n\")\n",
        "\n",
        "    openai_arguments = generate_human_like_arguments(topic, debate_type)\n",
        "\n",
        "    print(\"AI's Arguments:\")\n",
        "    print(openai_arguments)\n",
        "\n",
        "    print(\"\\nNow, enter your counter-argument:\")\n",
        "    user_argument = input()\n",
        "\n",
        "    bias_analysis = analyze_bias(user_argument)\n",
        "    print(\"\\nBias Analysis:\", bias_analysis)\n",
        "\n",
        "    print(\"\\nAI's Rebuttal:\")\n",
        "    rebuttal = generate_human_like_arguments(f\"Counter {user_argument}\", debate_type)\n",
        "    print(rebuttal)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ai_debate()\n"
      ]
    }
  ]
}